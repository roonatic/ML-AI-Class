{"cells":[{"cell_type":"markdown","metadata":{"id":"tuc4mTLmWcqc"},"source":["## [IMDB](https://torchtext.readthedocs.io/en/latest/datasets.html#imdb)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://pytorch.org/get-started/previous-versions/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8338,"status":"ok","timestamp":1684245044298,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"1wBRW0BLzPQY","outputId":"46ee8132-3782-429d-e7e1-80661182737b"},"outputs":[],"source":["# CUDA 11.8\n","#conda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 pytorch-cuda=11.8 -c pytorch -c nvidia\n","# CUDA 12.1\n","!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n","#conda install pytorch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 cpuonly -c pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install torchtext"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4867,"status":"ok","timestamp":1684245049163,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"FxPht0lQWcqd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/.local/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/ubuntu/.local/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/ubuntu/.local/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/home/ubuntu/.local/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","import numpy as np\n","from torchtext.datasets import IMDB\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader\n","from typing import List, Tuple"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684245049163,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"yGJjj0Vf8QgC"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684245049163,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"7ZENs36SWcqd"},"outputs":[],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684245049163,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"U3v40mf4Wcqd"},"outputs":[],"source":["!pip install torchdata portalocker"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["tokenizer = get_tokenizer(\"basic_english\")\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32453,"status":"ok","timestamp":1684245081612,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"jDng-po0Wcqd","outputId":"46acc01d-eaaf-4bc1-b916-44f5768f98e7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/.local/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n","################################################################################\n","WARNING!\n","The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n","future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n","to learn more and leave feedback.\n","################################################################################\n","\n","  deprecation_warning()\n"]}],"source":["train_iter, test_iter = IMDB(split=('train', 'test'))\n","text_vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","text_vocab.set_default_index(text_vocab[\"<unk>\"])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_iter, test_iter = IMDB(split=('train', 'test'))"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684245081612,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"cNXs8I0OWcqd","outputId":"39368cb4-545e-41fc-8919-06696679e7d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"]}],"source":["for label, text in train_iter:\n","    print(label)\n","    print(text)\n","    break"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{1, 2}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_iter, test_iter = IMDB(split=('train', 'test'))\n","labels = [a for a, b in list(train_iter)]\n","labels = set(labels)\n","labels\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1684245081613,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"LUBWiQ4NWcqf","outputId":"61f879ad-ed68-4c16-c34c-bb24f1be5a32"},"outputs":[],"source":["def process_text(text, tokenizer):\n","    return torch.tensor(text_vocab(tokenizer(text)), dtype=torch.long)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684245081613,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"rrjpkInJWcqf","outputId":"226742a8-0d25-4e17-f024-ea27d0d1497e"},"outputs":[],"source":["# collate 함수에서 text를 procesor로 tensor 변환\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for _label, _text in batch:\n","        label_list.append(int(_label))\n","        text_list.append(process_text(_text, tokenizer))\n","\n","    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=text_vocab['<pad>'])\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    return text_list, label_list"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684245081613,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"e876PuSuWcqf","outputId":"e25ba057-83b9-4532-c1f9-1ebf7c909896"},"outputs":[],"source":["batch_size = 4\n","\n","# IteratableDataset이므로 그대로 전달해도 됨, 다만 len()는 사용할 수 없음\n","train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n","test_loader = DataLoader(test_iter, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684245081613,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"VXhcShBJWcqf"},"outputs":[],"source":["#print('훈련 샘플의 개수 : {}'.format(len(train_loader)))\n","#print('테스트 샘플의 개수 : {}'.format(len(test_loader)))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684245081614,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"hVTtvBPWWcqf","outputId":"bbd1eb5d-b00d-4e05-edad-8ec2ba7115a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[  13, 5111,  443,  ...,    0,    0,    0],\n","        [  88,  121,    3,  ...,    0,    0,    0],\n","        [  12,   96,    8,  ...,    0,    0,    0],\n","        [ 107,    5, 4702,  ...,  406, 5868,    2]]) tensor([1, 1, 1, 1])\n"]}],"source":["for text, label in train_loader:\n","    print(text, label)\n","    break"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684245081614,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"-axauZtFWcqf","outputId":"10cdf909-2fc0-4365-aac3-839def8c54ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 437]) torch.Size([4])\n"]}],"source":["print(text.shape, label.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684245085555,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"-GZR4VD4tURC"},"outputs":[],"source":["embed_dim = 128\n","n_layers = 3\n","n_vocab = len(text_vocab.get_stoi())\n","\n","hidden_size = 256\n","output_size = len(labels)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684245085555,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"vVbGL7j-tbk4"},"outputs":[],"source":["class IMDBModel(nn.Module):\n","    def __init__(self, embed_dim, hidden_size, output_size, n_layers, device):\n","        super(IMDBModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.device = device\n","\n","        self.embed = nn.Embedding(n_vocab, embed_dim)\n","        self.rnn = nn.GRU(input_size=embed_dim, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_size, output_size)\n","        )        \n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        hidden = torch.zeros(n_layers, batch_size, self.hidden_size).to(device)  \n","        x = self.embed(x)\n","        x, hidden = self.rnn(x, hidden)\n","        x = x[:,-1,:]\n","        output = self.fc(x)\n","        return output"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7702,"status":"ok","timestamp":1684245093252,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"hZttIEJ0tgWP","outputId":"f0499986-8797-45c3-e8a9-3c9a8f6eb41f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.1571, -0.0586],\n","        [-0.1571, -0.0586],\n","        [-0.1571, -0.0586],\n","        [-0.1330,  0.0577]], device='cuda:0')\n","tensor([1, 1, 1, 1]) tensor([1, 1, 1, 1])\n"]}],"source":["import numpy as np\n","\n","model = IMDBModel(embed_dim, hidden_size, output_size, n_layers, device).to(device)\n","X = torch.LongTensor(text).to(device)\n","with torch.no_grad():\n","    y_pred = model(X)\n","    print(y_pred)\n","    print(np.argmax(y_pred.cpu(), axis=1), label)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356663,"status":"ok","timestamp":1684245449913,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"8XyoZaCNtxaM","outputId":"a2a2ece5-b4f8-4fa2-9d8d-e4ee2cfd0931"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1th training loss: 0.004702317092817757 test loss: 0.006076825556755066, accuracy: 50.0\n","epoch 2th training loss: 0.004572136507864343 test loss: 0.006340211351513862, accuracy: 62.619998931884766\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(text)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(label \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 24\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m X_train\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y, y_train)\u001b[38;5;241m.\u001b[39msum()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mIMDBModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n_layers, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(x)\n\u001b[0;32m---> 18\u001b[0m x, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1133\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1137\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["batch_size = 120\n","model = IMDBModel(embed_dim, hidden_size, output_size, n_layers, device).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","n_epochs = 10 # 10회 학습\n","\n","list_training_loss = []\n","list_test_loss = []\n","\n","\n","for epoch in range(n_epochs):\n","    n_train = 0\n","    train_loss = 0\n","\n","    train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n","    test_loader = DataLoader(test_iter, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n","\n","    model.train()\n","    for text, label in train_loader:\n","        X_train = torch.LongTensor(text).to(device)\n","        y_train = torch.LongTensor(label - 1).to(device)\n","        y = model(X_train)\n","        del X_train\n","        loss = criterion(y, y_train).sum()\n","        train_loss += loss.data.cpu().numpy()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        n_train += len(y_train)\n","        del y_train\n","\n","#    if (epoch + 1) % 10 == 0:\n","    if True:\n","        model.eval()\n","        n_test = 0\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        \n","        for text, label in test_loader:\n","            X_test = torch.LongTensor(text).to(device)\n","            y_test = torch.LongTensor(label - 1).to(device)\n","            y_pred = model(X_test)\n","            idx_pred = torch.max(y_pred, 1)[1]\n","            del X_test\n","            loss = criterion(y_pred, y_test).sum()\n","            test_loss += loss.data.cpu().numpy()\n","            n_test += len(y_test)\n","            correct += (idx_pred == y_test).sum()\n","            del y_test\n","\n","        accuracy = correct * 100 / n_test\n","        print('epoch {}th training loss: {} test loss: {}, accuracy: {}'.format(\n","                epoch + 1, train_loss / n_train, test_loss / n_test,\n","                accuracy\n","            ))\n","        list_training_loss.append(train_loss / n_train)\n","        list_test_loss.append(test_loss / n_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":764,"status":"ok","timestamp":1684245450670,"user":{"displayName":"노규남","userId":"01550891333411762759"},"user_tz":-540},"id":"xDAx3a1yt0iR","outputId":"883afb6f-2dbe-48d5-9144-138356bd58b3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(list_training_loss, label='training')\n","plt.plot(list_test_loss, label='test')\n","plt.legend()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
